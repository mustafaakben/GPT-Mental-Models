# Mental Model Theory â€” Math Notes (working notepad)

Author: Codex assistant
Date: 2025-10-19
Scope: Notes distilled from Mental Model Math.md to guide implementation of a symbolic core and a differentiable ML model (DMMC).

---

## 1) Quick Recap (my understanding)
- Reasoning state is a finite set of partial models (â€œworldletsâ€) Î£ = {m_k}. Each worldlet stores explicit truths (and optionally negative literals) while leaving others unknown (âŠ¥).
- Two evaluation regimes:
  - Inspection (fast): read off monotone consequences from Ï„(m) (truth-only projection) via lower-bound valuation v_m.
  - Deliberation (search): expand missing possibilities (E) and test necessity by supervaluation over completions Comp(E(Î£)).
- Premises are constructed via minimal generator G(Â·) and merged with âŠ— (consistent union product). Expansion E(Â·) makes meaning fully explicit (e.g., conditional adds {Â¬Ï‡}).
- ML angle (DMMC): latent K worldlets in {1,0,âŠ¥}, with an encoder E_Î¸(x) â†’ Z, an inspection head for monotone queries, an expansion/search module (symbolic or learned), and losses that encourage MMT priors (few models, truth-only sparsity, shallow expansion when possible).

---

## 2) Core Objects & Semantics (to encode)
- Literals/atoms: ğ’œ = {pâ‚â€¦p_n}, Lit = {p, Â¬p}.
- Worldlet m âŠ† Lit (consistent); projection Ï„(m) = {p âˆˆ ğ’œ | p âˆˆ m}.
- Completions Comp(m): classical total assignments consistent with all signed facts in m; Comp(Î£) = â‹ƒ_m Comp(m).
- Supervaluation:
  - Necessary: Î£ âŠ¨ Ï† â‡” âˆ€mâˆˆÎ£ âˆ€sâˆˆComp(m): s âŠ¨ Ï†.
  - Possible: Î£ âŠ¨â—‡ Ï† â‡” âˆƒmâˆˆÎ£ âˆƒsâˆˆComp(m): s âŠ¨ Ï†.
- Inspection (monotone Ïˆ): Î£ âŠ¢_ins Ïˆ iff âˆ€mâˆˆÎ£: v_m âŠ¨ Ïˆ, where v_m(p)=1 if pâˆˆÏ„(m) else 0. Lemma: inspection â‡’ necessity for monotone Ïˆ.

---

## 3) Construction Operators (to implement)
- Merge (âŠ—): pairwise unions of worldlets, filter inconsistent.
- Generator G(Â·) (minimal, truth-only):
  - G(p) = {{p}}; G(Â¬Ï‡) = {{Â¬Ï‡}}.
  - G(Ï‡âˆ§Ïˆ) = {m_Ï‡ âˆª m_Ïˆ}.
  - G(Ï‡âˆ¨Ïˆ) = G(Ï‡) âˆª G(Ïˆ) âˆª optional {m_Ï‡ âˆª m_Ïˆ}.
  - G(Ï‡â†’Ïˆ) = {{Ï‡, Ïˆ}} (not-Ï‡ left implicit initially).
  - G(Ï‡â†”Ïˆ) = {{Ï‡, Ïˆ}, {Â¬Ï‡, Â¬Ïˆ}}.
- Expansion E(Â·) (explicitate omitted possibilities):
  - E respects connectives; crucially E(G(Ï‡â†’Ïˆ)) = {{Ï‡, Ïˆ}, {Â¬Ï‡}}.
  - Thm: Comp(E(Î£_P)) equals the set of classical models of premises P.

---

## 4) Quantifiers/Counts/Modality (options layer)
- Set-worldlets for FOL: store positive witnesses Wâº and structural set constraints C (e.g., S_A âŠ† S_C). Domain D unknown but non-empty.
- G(âˆƒx P(x)) â†’ witness P(a). G(âˆ€x (Aâ†’C)) â†’ subset constraint only. â€œMost(A,C)â€ â†’ |Aâˆ©C| > |A\C|.
- Modal: â–¡Ï† iff Î£ âŠ¨ Ï†; â—‡Ï† iff Î£ âŠ¨â—‡ Ï†. Causality via coupled completions (actual vs. counterfactual) with sufficiency + tendency constraints.

---

## 5) Complexity & Cost (priors to encode)
- Cost(Î£) = Î±Â·|Î£| + Î²Â·avg unk(m). Pressure toward â€œone model is better than manyâ€ and minimal explicit info unless needed.
- Case-split bound: |E(G(Ï†))| â‰¤ 2^(#âˆ¨) Â· 2^(#â†’); |G(Ï†)| â‰¤ 2^(#âˆ¨).

---

## 6) Probabilities from Possibilities
- Ï€(m) over worldlets; within each m, distribute over Comp(m) (assume near-uniform on unknowns).
- P(Ï† | Î£, Ï€) = âˆ‘_m Ï€(m) Â· P_{sâˆ¼Î¼_m}[s âŠ¨ Ï†].
- Rough-average property for disjunction when joint worldlet omitted: P(Aâˆ¨B) â‰ˆ Â½(P(A)+P(B)).

---

## 7) DMMC (Differentiable Mental-Model Calculus) â€” Implementation Sketch
- Latent state: Z âˆˆ {1,0,âŠ¥}^{KÃ—n} via logits â„“ and Gumbel-Softmax/straight-through.
- Modules:
  - Encoder E_Î¸(x) â†’ Z (text/vision to worldlets).
  - Inspector: evaluates monotone queries with v_m (or relaxed \hat v_m) and t-norm/conorm.
  - Expander/Searcher: apply E-templates on demand; SAT/SMT or MILP for counterexample search over Comp(E(Î£)).
  - Probability head: softmax over worldlet scores â†’ Ï€.
- Losses:
  - Task CE/Brier on necessary/possible or QA labels via supervaluation on expanded Î£.
  - MMT prior: Î»â‚Â·|Î£| + Î»â‚‚Â·avg unk(m) + Î»â‚ƒÂ·ExpCost.
  - Optional ST estimator for non-differentiable expansion decisions.

---

## 8) Design Choices To Lock Down (please confirm)
1) Disjunction in G by default:
- Option A (MMT-minimal): G(Aâˆ¨B) = {{A},{B}} only; joint added only by expansion or encoder.
- Option B (richer explicit): include joint {{A,B}} at construction. Default? (Leaning A for stronger MMT prior.)

2) Negatives inside worldlets:
- Keep Â¬p in m but ignore at inspection (via Ï„). Activates during deliberation/search. Confirm yes.

3) Expansion budget/strategy:
- Depth-1 for conditionals first; selective expansion guided by learned heuristics and Cost(Î£). Define a scheduler.

4) Monotone-formula detector:
- Implement syntactic checker to route queries to inspector vs. searcher. Edge: literals vs. mixed forms.

5) K (max worldlets):
- Start small (K=3â€“5) for toy tasks; scale as needed. Regularize to actually use fewer.

6) Quantifiers layer:
- Initial version: propositional only; add set-worldlets later. If needed early, weâ€™ll encode âˆƒ (witness) and âˆ€ (subset constraint) first.

7) Probabilities:
- Use Ï€ over worldlets; within-worldlet completion uniformity is an approximation. Consider temperature to control spread.

8) Backend for search:
- Start with PySAT/Z3 via a thin encoding of Comp(E(Î£)). Provide fallbacks to pure Python bit-vector SAT for portability.

---

## 9) Edge Cases & Pitfalls (watchouts)
- Inconsistency during âŠ—: need fast mask to drop pairs where p and Â¬p collide.
- Double counting in probability if joint worldlet exists alongside singles; ensure worldlet priors Ï€ are normalized and completions within each m do not overlap across worldlets (they can, but Ï€ handles mixture correctly).
- Non-monotone queries at inspection lead to unsoundness; must gate via monotone check.
- Quantifiers: domain variability can make supervaluation subtle; fix D or constrain via K knowledge base to avoid degenerate completions.
- Straight-through estimators can destabilize; cap expansion steps and use auxiliary predictors of counterexamples.

---

## 10) Minimal v0 Plan (symbolic core)
- Types: Atom, Literal, Worldlet (bitset of {1,0,âŠ¥}), ModelSet Î£ (list of worldlets).
- Ops: `merge_otimes(Î£â‚, Î£â‚‚)`, `generator_G(formula)`, `expand_E(Î£)`, `tau(worldlet)`, `inspect_monotone(Î£, Ïˆ)`, `supervaluate(Î£, Ï†)`, `counterexample(P, Î¸)`.
- SAT bridge: encode Comp(E(Î£)) as CNF constraints; ask SAT for s âŠ­ Î¸.
- Tests: MP easy; MT requires expansion; AC/DN invalid; disjunction rough-average probability check.

---

## 11) Datasets & Evaluation Ideas
- scikit-learn classics: Iris, Wine, Breast Cancer (Wisconsin diagnostic). Primary target for first experiments.
- Synthetic propositional sets matching classic MMT tasks (MP/MT/AC/DN, biconditionals).
- Syllogisms/quantifiers (All/Some/Most) with set diagrams.
- Natural language conditionals from existing logic QA datasets (scaled-down first).
- Metrics: classification accuracy/F1/AUROC where applicable, calibration (ECE/Brier), plus MMT metrics: |Î£|, avg unk(m), expansion depth, inspection/share ratio, time per query.

---

## 12) Open Questions For You
- Target domain first (textual logic, visual reasoning, planning)?
- Preferred default for G(Aâˆ¨B) (Option A vs. B)?
- Do we include causal layer in v1 or postpone?
- Any constraints on K or latency that shape expansion strategy?
- Should we support probabilities in v1 or ship deterministic first?

---

## 13) Next Steps (proposed)
1) Finalize the design choices above (especially disjunction default, K, and expansion budget).
2) Implement symbolic core (G, âŠ—, E, Ï„, inspection, supervaluation) with unit tests.
3) Add SAT-backed counterexample search and necessity checker.
4) Expose a simple API: `judge(Premises, Query) â†’ {necessary|possible|impossible}` plus traces.
5) If desired, scaffold DMMC encoder stubs and a tiny training loop to learn Ï€ and worldlet logits from labeled tasks.

---

## 14) Glossary (quick refs)
- Ï„(m): truth-only projection (drops negatives).
- v_m: lower-bound valuation used for inspection.
- G(Â·): generator (minimal explicit worldlets from a formula).
- E(Â·): expansion (adds omitted possibilities demanded by full meaning).
- âŠ—: consistent union merge of premises.
- Comp(m): classical completions consistent with m.
- Cost(Î£): Î±Â·|Î£| + Î²Â·avg unk(m) (+ Î»Â·ExpCost in learning).

---

## 15) scikit-learn Benchmarks (Iris, Wine, Breast Cancer)
Note: assuming â€œwiseâ€ refers to scikit-learnâ€™s Wine dataset.

**Datasets**
- Iris: 150 samples, 4 numeric features, 3 classes.
- Wine: 178 samples, 13 numeric features, 3 classes.
- Breast Cancer (Wisconsin diagnostic): 569 samples, 30 numeric features, binary classes.

**Goals**
- Use these as sanity-check benchmarks to validate DMMCâ€™s inspection vs. deliberation behavior and the effect of the MMT prior (few worldlets, low expansion).
- Compare to standard baselines and inspect model traces for interpretability.

**Feature â†’ Atom mapping (propositionalization)**
- Discretize numeric features into monotone threshold atoms per feature j:
  - Quantile thresholds: t âˆˆ {q20, q40, q60, q80} â‡’ atoms p_{j,t}: x_j â‰¥ t.
  - Supervised thresholds: 1â€“3 splits from a univariate DecisionTree (stumps) per feature.
  - Optionally k-bin (kâˆˆ{3,5}); encode as cumulative atoms to preserve monotonicity.
- Each sample â†’ truth assignment over atoms; unknowns (âŠ¥) if a feature missing (not expected here).

**Premise/Query design for MMT evaluation**
- Generate simple rules from thresholds (e.g., x_j â‰¥ t â†’ class=c) to test MP vs. MT dynamics.
- For multiclass, use one-vs-rest class atoms C_c and encode premises as implications; evaluate necessity vs. possibility of class labels.

**Baselines**
- LogisticRegression, LinearSVC/RBF-SVC, DecisionTree, RandomForest; measure accuracy/AUROC and calibration.
- Ensure DMMC is not disadvantaged: tune thresholds and K fairly via CV.

**Protocol**
- Stratified 5Ã— (train/val/test = 70/15/15) with fixed seeds; early dev can use 80/20 split.
- Report meanÂ±std across runs for: accuracy/F1 (macro), AUROC (binary), ECE/Brier (probabilistic), and MMT metrics (|Î£|, unk, expansions, inspection% ).

**Loading hooks (sklearn)**
- `from sklearn.datasets import load_iris, load_wine, load_breast_cancer`
- `X, y = load_iris(return_X_y=True)`, similarly for wine and breast_cancer.

**Integration sketch**
1) Build `discretize(X)` â†’ boolean atom matrix A and threshold dictionary.
2) Encode premises from atoms to initiate Î£ via G and âŠ—; set class atoms as queries.
3) Run inspection for monotone queries; escalate to expansion/search when needed.
4) Compare predictions and compute metrics; log Î£ size and expansion counts.

**Open knobs**
- K (worldlets): start at 3 for Iris/Wine, 5â€“7 for Breast Cancer.
- Threshold count per feature (1â€“4); regularize via Î» terms to keep Î£ small and expansions rare.

