# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

This repository implements **Mental Models Theory (MMT)** as a neural network framework for reasoning and classification. The project translates cognitive science principles—specifically Johnson-Laird's Mental Models Theory—into a computational, machine learning-ready architecture.

### Core Concept

MMT proposes that humans reason by constructing small, structural simulations ("worldlets") of possible situations rather than applying logical rules. Key principles:

- **Worldlets as possibilities**: Each worldlet represents a possible state of the world
- **Truth-only principle**: Models explicitly represent what is true, leaving false information implicit ("mental footnotes")
- **Two-stage inference**:
  - Fast inspection (System-1): Read off consequences from explicit truths
  - Deliberative search (System-2): Expand implicit possibilities and search for counterexamples
- **Complexity cost**: Reasoning difficulty scales with the number of models required ("one model is better than many")

## Repository Structure

### Documentation Files

- [Mental Model Theory.md](Mental Model Theory.md): Comprehensive overview of MMT cognitive theory, including representations, algorithms, and empirical evidence
- [Mental Model Math.md](Mental Model Math.md): Mathematical formalization of MMT including worldlet semantics, operators (G, E, ⊗), supervaluation, and proofs
- [notepad.md](notepad.md): Working notes with implementation decisions, design choices, and benchmark plans

### Python Implementations (Evolutionary Versions)

The codebase contains three evolutionary versions of the implementation:

1. **MentalModel-v001.py**: Latest stable version
   - Implements MMTNet: worldlet-based tabular classifier with gated negations
   - Benchmarks against sklearn baselines (SVM, RandomForest, GradientBoosting)
   - Tests on Iris, Wine, and Breast Cancer datasets
   - Features: positive-weight MLPs, truth-only bias, Possibility/Necessity aggregation

2. **MentalModel-v002.py**: Multi-strategy atom framework
   - Vectorized implementation with 8 atom generation strategies
   - Includes synthetic datasets (XOR, moons, circles) plus real data
   - Grid search over 14 atom strategy configurations
   - Performance optimized with parallel execution and progress bars

3. **MentalModel-v003a.py**: Simplified production version (appears to be the most mature)
   - Clean MMTNet implementation for tabular data
   - 5-fold stratified cross-validation
   - Confusion matrices and classification reports
   - Outputs results as CSVs and PNG visualizations

**Note**: v003a appears to be the current reference implementation. Earlier versions (v001, v002) contain experimental features and optimizations.

## Running the Code

### Prerequisites

Install dependencies from [requirements.txt](requirements.txt):

```bash
pip install -r requirements.txt
```

Requirements include PyTorch 2.0+, scikit-learn, numpy, and matplotlib.

### Basic Usage

Run the latest benchmark:

```bash
python MentalModel-v003a.py
```

This will:
- Load Iris, Wine, and Breast Cancer datasets
- Run 5-fold cross-validation for all models
- Perform 80/20 holdout evaluation
- Generate confusion matrices as PNGs in `./results/`
- Save summary CSVs to `./results/`

### Configuration

Each Python file contains configurable globals at the top:

**MentalModel-v003a.py** (recommended):
- `SEED`: Random seed (auto-generated by default)
- `N_SPLITS`: Number of CV folds (default: 5)
- `TEST_SIZE`: Holdout test split (default: 0.2)
- `MMTNetConfig`: Hyperparameters for MMTNet
  - `K`: Number of worldlets (default: 3)
  - `h`: Hidden layer size (default: 64)
  - `lr`: Learning rate (default: 1e-3)
  - `epochs`: Max training epochs (default: 200)
  - `lambda_K`: Worldlet compute penalty (default: 0.01)
  - `lambda_eta`: Negation gate penalty (default: 0.05)

### Output Files

All results are saved to `./results/`:
- `cv_summary.csv`: Cross-validation metrics (accuracy, F1-macro, ROC-AUC)
- `holdout_summary.csv`: Holdout test metrics and training times
- `confusion_<dataset>_<model>.png`: Normalized confusion matrices

## Architecture Details

### MMTNet Implementation

**Input Processing:**
- Augments input features: `[x, -x]` to represent both positive features and their negations
- Negations act as "mental footnotes" controlled by learned gates

**Worldlet Layer:**
- K parallel worldlets, each representing a possible model
- Per-worldlet gating: `m_k ∈ [0,1]^(2d)` selects which features to use
- Positive-weight MLPs: All weights are softplus-reparameterized to be non-negative (enforces monotonicity in line with MMT's inspection mechanism)

**Aggregation:**
- Computes per-worldlet class probabilities: `p_k(x)`
- Worldlet weighting: `π(x)` from small gating network (learns which worldlets are active per sample)
- **Possibility** (noisy-OR): `q_poss[c] = 1 - ∏_k (1 - π_k * p_k[c])`
- **Necessity** (geometric mean): `q_nec[c] = ∏_k p_k[c]^π_k`
- Final output: learned mixture of Possibility and Necessity, normalized

**Regularization (MMT Priors):**
- `lambda_K * Peff_mean`: Penalizes effective number of active worldlets (encourages sparsity)
- `lambda_eta * neg_gate_mean`: Penalizes negation gate activation (truth-only bias)
- `lambda_gate * gate_sparsity`: Penalizes overall feature gate activation

### Training

- Uses Adam optimizer with ReduceLROnPlateau scheduler
- Early stopping based on validation loss (default patience: 20 epochs)
- Data always standardized (StandardScaler) for MMTNet
- Cross-entropy loss + MMT regularization terms

## Key Mathematical Operators

From [Mental Model Math.md](Mental Model Math.md):

- **G(φ)**: Generator that builds minimal worldlets from formulas (truth-only principle)
- **⊗**: Consistent union merge operator for combining premises
- **E(Σ)**: Expansion operator that makes implicit possibilities explicit
- **τ(m)**: Truth-only projection (extracts positive literals)
- **Comp(m)**: Classical completions of a partial worldlet
- **Supervaluation**: Determines necessity (∀ completions) vs possibility (∃ completion)

## Development Notes

### Testing Strategy

The codebase uses scikit-learn toy datasets as sanity checks:
- **Iris**: 150 samples, 4 features, 3 classes (baseline test)
- **Wine**: 178 samples, 13 features, 3 classes (moderate complexity)
- **Breast Cancer**: 569 samples, 30 features, 2 classes (high-dimensional binary)

MMTNet should be competitive with baselines while demonstrating interpretable worldlet structure.

### Known Characteristics

- MMTNet performance depends heavily on K (number of worldlets):
  - Too few: underfitting
  - Too many: overfitting and slow convergence
  - Start with K=3 for simple datasets, K=5-7 for complex ones
- Regularization hyperparameters (λ_K, λ_eta, λ_gate) are crucial for balancing task performance with MMT cognitive plausibility
- The random seed significantly affects results due to small dataset sizes

### Common Commands

```bash
# Run the main benchmark
python MentalModel-v003a.py

# Check results
ls -lh results/
cat results/cv_summary.csv

# View confusion matrices
open results/confusion_*.png  # macOS
xdg-open results/confusion_*.png  # Linux
```

### GPU Usage

PyTorch will automatically use CUDA if available. The code includes device detection:

```python
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
```

For small datasets (Iris, Wine, Breast Cancer), CPU is sufficient. GPU becomes important for larger-scale experiments.

## Theoretical Background

This implementation is grounded in formal cognitive science research. Key theoretical components:

1. **Conditionals**: "If A then C" initially represented as one explicit worldlet `{A, C}` with implicit `{¬A}` cases
2. **Disjunctions**: "A or B" represented as separate worldlets `{A}`, `{B}`, optionally `{A, B}`
3. **Quantifiers** (future work): Universal/existential encoded as set relations and structural constraints
4. **Probability from possibilities**: Uncertainty derived by apportioning probability across worldlets, not from truth-functional rules

The neural implementation approximates these symbolic operations through learned representations while preserving core MMT principles.

## Future Directions

From [notepad.md](notepad.md):

1. **Symbolic core**: Exact G, ⊗, E operations with SAT/SMT backend for counterexample search
2. **Natural language**: Encoder to map text to worldlets
3. **Visual reasoning**: Image-to-worldlet architectures
4. **Quantifiers**: Add set-worldlet layer for FOL reasoning
5. **Causal reasoning**: Implement Enable/Cause operators with counterfactual worldlets
6. **Differentiable search**: Replace discrete SAT with differentiable counterexample detection

## Git Workflow

Current branch: `main`

The repository tracks:
- Theoretical documentation (Markdown files)
- Python implementations (v001, v002, v003a)
- Requirements file
- Results directory (generated outputs)

Deleted files:
- `MMT.py` (replaced by versioned implementations)
